Although successful blinding removes the threat the John Henry effect poses to internal validity, a threat to external validity remains: it is difficult to predict the outcome of the treatment once the subjects are no longer aware of being observedMoreover, blinding alone does not mitigate the physician effect: if, for example, experienced physicians are more likely to treat patients in the experimental group, we would expect the effectiveness of the treatment to be overestimated. We turn now to the ways in which physician effects arise in AI RCT trials. The results of an AI RCT would be misleading if clinicians receiving AI assistance tended to be more experienced, specialized, or less burdened than those in the control group. We would expect patients treated by more experienced physicians to have better outcomes, regardless of AI assistance. Moreover, experience may interact with the treatment in unexpected ways: more experienced clinicians may react very differently to AI assistance than their less experienced colleagues. For example, Philipp Tschandl et al. (2020) studied dermatologists interacting with an image-based AI for diagnosing skin cancer. They found that less experienced clinicians tended to accept AI-based support that contradicted their initial diagnosis even if they were very confident. More experienced clinicians, by contrast, tended to change their diagnoses to agree with the AI only when they were not confident. Although less experienced clinicians benefited significantly from AI assistance, experienced clinicians benefited only marginally. However, in cases when they were antecedently confident about their diagnosis, experienced clinicians performed worse with AI support— in the rare cases when they changed their diagnoses to agree with the AI, they tended to be led astray. Assuming the results of Tschandl et al. are representative, AI trials would not be probative about the usefulness of AI assistance if senior clinicians were over-represented in the intervention group: treatment decisions would be hardly changed. Conversely, if junior clinicians were over-represented in the intervention group, the trial would overestimate the benefits of AI, since adverse effects on experienced clinicians would rarely be observed. To adequately control for these potential physician effects, it is important for any study to be clear about the distribution of experience and expertise among clinicians in the experimental and control groups, especially since the number of clinicians involved tends to be very small. Random assignment can mitigate the effect of physician effects, but a lot depends on the details of randomization. For example, researchers could randomize each physician to either always or never receive AI assistance. The trouble with this scheme is that, in the usual case when only a few physicians are participating in the trial, small sample- size effects may predominate: if only three senior and three junior clinicians are participating in the trial, it is not unlikely that all the senior physicians are assigned to always receive AI support. Moreover, in such a scheme, it is not possible to compare an individual physician's performance with and without algorithmic assistance—only the average difference between patient groups can be measured. A more promising scheme first Thomas Grote and Konstantin Genin " 7 Philosophy of Medicine " DOI 10.5195/POM.2021.27 " Volume 2 " pp. 1-15 assigns patients to physicians and then randomly assigns each unique physician-patient pair to treatment or control. This scheme ensures independence between clinician experience and treatment assignment and also enables analysts to compare individual physicians to themselves with and without AI assistance. In this way, each physician serves as her own control. Although this is relatively standard practice in drug trials, it is unclear if it is followed in existing AI RCTs. Only Wang et al. (2019) include comparative statistics on clinician expertise, and this is dropped in Wang et al. (2020). Investigators should be encouraged to be clear on this matter. Although physician effects can be mitigated by appropriate randomization, John Henry and related effects must be dealt with in some other way. Nagendran et al. (2020) and Yuichi Mori, Shin-ei Kudo and Masashi Misawa (2020) call for increasing use of double-blind designs in AI RCTs. Double-blind AI RCTs are indeed rare: so far, only Wang et al. (2020) have performed such a study. The usual methodological justification for blinding the clinician is to ensure that preconceived ideas of the investigator are not important to patient outcomes (Friedman, Furberg and DeMets 2010). Of course, blinding does not remove the influence of preconceived ideas, but it does ensure that their effects are not preponderant in any single group. For example, if clinicians are hostile to AI assistance, they may unconsciously sabotage it. If they are uncritical boosters of AI, they may put more effort in when they receive AI assistance than they do without it. To ensure that these effects are not concentrated in the experimental or control group, the clinicians could be blinded to the use of AI assistance. This is prima facie difficult: how could you not know if you were receiving AI assistance? One way this could be achieved is with a Turing-style design. For example, in AI-assisted colonoscopy, the AI displays alerts for adenoma structures that appear in the visual field. In a Turing-style design a human clinician sitting in a separate room could generate the alerts instead. In such a design, the operating endoscopist would not know whether a human or a machine were generating the alert. Of course, the results of such a trial would only bear on how AI assistance compares with the human assistance and not on whether it is better than no assistance at all. For this reason, it may be desirable to run a three-way trial in which a third group of patients is randomized to receive care without additional assistance. Wang et al. (2020) motivate their double-blind design by appeal to a kind of John Henry effect: One major limitation of the existing non-blinded studies was the introduction of operational bias, because operating endoscopists using the CADe system might be more vigilant because of a competitive spirit or relax and rely on the CADe system. In both cases, the effectiveness of the CADe system might be overestimated or underestimated. (344) Wang et al. should be commended for their attention to these potential biases. However, their attempts to account for it are, in our opinion, counterproductive. To mask the endoscopists, Wang et al. developed a "sham system" that "simulated alert boxes on polyp- like non-polyp structures (e.g., bubbles, faeces, undigested debris, and wrinkled mucosa) without tracking actual polyps during the colonoscopy" (2020, 345). Then, the output of both the CADe and the sham system was shown on a second monitor, which was visible only to an observing senior endoscopist and not the operating endoscopist. In both groups, "the observer was responsible for reporting the location of any visible alert box for the endoscopist with a laser pointer on the primary screen." As the authors note, the very fact of being observed might have improved inspection technique and "motivated the Randomised Controlled Trials in Medical AI: A Methodological Critique " 8 Philosophy of Medicine " DOI 10.5195/POM.2021.27 " Volume 2 " pp. 1-15 competitive spirit" of the operating endoscopists—that may go some way towards equalizing novelty effects across the two groups. However, endoscopists receiving AI assistance were compared to endoscopists distracted with irrelevant and deliberately misleading laser alerts. The tendency of this design is to exaggerate the helpfulness of AI assistance by comparing it, not with the absence of AI assistance, but with the presence of algorithmic sabotage. This sort of design is convoluted and counterproductive and it also raises ethical concerns, as it imposes unnecessary risks on patients. If a Turing-style design is impractical, endoscopists receiving AI assistance could have been compared with endoscopists stimulated by the supervision of a senior colleague. Of course, that would mean that AI- assisted treatment would have to pass a rather severe test: it would have to be an improvement over two clinicians working in tandem. In yet another approach, the AI would assist with every patient, but randomize how "helpful" it is going to be—for example, by reducing or increasing the number of interventions. In this way, all clinicians could be made to anticipate AI assistance. Then, endoscopy sessions receiving significant assistance could be compared to those receiving minimal assistance—all clinicians would then be motivated by (perceived) algorithmic competition. The benefits of the Wang et al. design could be had without distracting and misleading practising clinicians with flashing lights. So far, we have been concerned with potential threats to internal validity. But even studies that provide an excellent, unbiased estimate of the average treatment effect in the trial population may fail to generalize outside of the context of the trial. In what follows, we consider what features of AI RCTs threaten their external validity. In the context of a trial, clinicians are interacting with an untested AI system. They may regard it critically or be moved to greater concentration by a spirit of competition. This may give a temporary and artificial advantage to clinicians in the experimental group. In a short trial, researchers will oversample the period in which clinicians are still adjusting to the new system and before they are able to use it effectively. They will not yet understand the AI system's basic capabilities and limitations, or its medical point of view: how severely it grades disease, or how to interpret its probabilistic output (Cai et al. 2019). Though present during the study, these novelty effects would subside outside of it. Were the AI to be widely adopted, clinicians would be interacting with a "proven" system. They would have become acclimated to the AI. In the long run, they may learn to ignore it, or they may be coaxed into an over-reliance on its assistance (Park et al. 2020). A straightforward approach to dealing with novelty effects is to wait until they wash out: if AI trials run longer than a few weeks, clinicians will have time to acclimate themselves to the new system. Then, comparisons of the experiment and control outcomes could investigate how differences in outcomes evolve over time. This would give the most realistic picture of how the effect of AI assistance would evolve in a clinical setting. It is important to note that widespread adoption of AI systems may pre-empt the development of certain kinds of expertise. If junior clinicians are over-reliant on AI systems, they may never develop the mastery of their more senior colleagues. Junior clinicians may be inadvertently trained to uncritically imitate AI systems, instead of critically collaborating with them. In this way, AI systems may eventually be used by clinicians who are less confident, experienced, or critical than those on whom they were originally tested. These considerations argue in favour of long-run clinical trials that investigate both how AI assistance interacts with clinician experience and how these effects evolve over time. Over- reliance on AI assistance might be mitigated by algorithmic explanations. Tschandl et al. (2020) argue that explanations provided by the AI system (by way of a heatmap, which highlights regions of interest) play an important pedagogical role as clinicians progress from novice to expert (on explainable AI in medicine, see Erasmus, Brunet and Fisher 2020; Thomas Grote and Konstantin Genin " 9 Philosophy of Medicine " DOI 10.5195/POM.2021.27 " Volume 2 " pp. 1-15 Sullivan, forthcoming). Through the explanations, the clinicians learn to direct their attention to meaningful signs and symptoms. Of course, this assumes that the AI's judgements—as well as its "reasons" are themselves reliable. In any case, it is better that clinicians understand the reasoning of the AI system, so that they can adjudicate any disputes with standard clinical reasoning. So far, medical AI RCTs have not investigated to what extent explainability improves the diagnostic support of the AI system. We believe that it is important to close this research gap, to get better evidence on when and what sort of explanations are required to improve the interplay of AI systems and clinicians. Additionally, researchers should take care that the introduction of AI assistance does not induce a survival bias. For example, Google Health deployed the Gulshan et al. (2016) algorithm for detecting diabetic retinopathy in retinal photographs in eleven clinics in Thailand (Beede et al. 2020). To ensure accuracy, the system accepted only high-quality images. Since many images were taken in poor lighting conditions, more than a fifth were rejected. Patients with rejected images were asked to come back another day. Poor internet connections also caused problems with uploading the images. This study was not an RCT, but if it had been, it is easy to imagine how these problems would induce survival bias: patients at well-equipped clinics with stable internet connections would be over- represented in the experimental group. In all likelihood, richer patients would therefore also be over-represented, even if assignments were randomized. This would probably overestimate the effectiveness of AI assistance. AI trials should be vigilant with regard to these possibilities. If possible, they should include the intention-to-treat analysis as well as a per-protocol analysis. Researchers should justify their decisions to perform one or the other kind of analysis. The failure of Google Health to successfully implement the AI system also highlights some constraints with respect to the transferability of AI systems to different environments. While the relevant systems may work reliably in a state-of-the-art academic hospital, they may be useless for less well-equipped hospitals. The choice of which clinical endpoint to measure and compare has proven to be intricate. Ideally, what you want to establish in an RCT is that a given treatment had a meaningful effect on patient health (for example, whether there is an increase in the survival rate or recovery time). The problem with current AI systems is that they are neither a treatment in themselves, nor do they determine treatment on their own. What they do instead is to provide a secondary diagnostic opinion to the clinician. The role of the AI system is therefore causally upstream of treatment (see Lalumera and Fanti 2019 for similar concerns regarding medical imaging technologies). The decision of the AI system could be ignored by the clinician, or otherwise be irrelevant for the choice of treatment. This makes it tempting to choose a surrogate endpoint, such as diagnostic accuracy, as in Wang et al. (2020). What speaks in favour of diagnostic accuracy is that by getting the diagnosis correct, it spares patients an odyssey of further diagnostic tests—which in itself can be considered as a quality of life mprovement. However, relying on a surrogate endpoint may backfire. A particular worry is that the involvement of AI systems leads to overtreatment (Oren, Gersh and Bhatt 2020). While an AI system may spot tumours more accurately than even expert clinicians, these previously overlooked tumours may be clinically irrelevant. Wang et al. (2019, 2020) find that their system increases the detection of small and diminutive polyps, whose relevance for colorectal cancer prevention is debatable (Vleugels et al. 2017). However, once spotted, further interventions that are harmful to the patient are highly likely to follow, from biopsy to chemotherapy. Hence, merely focussing on surrogate endpoints is insufficient to establish the medical benefit of AI systems. Theoretically, the problem might be mitigated by using a more refined metric of diagnostic accuracy, which includes parsing Randomised Controlled Trials in Medical AI: A Methodological Critique " 10 Philosophy of Medicine " DOI 10.5195/POM.2021.27 " Volume 2 " pp. 1-15 which disease will impact patients, and which will not.7 However, making these sorts of prognoses is beyond what current image-based diagnostic AI systems are capable of. Although RCTs are necessary for testing the mettle of AI systems, they are not sufficient in and of themselves. A fundamental question any AI trial should be able to answer is how, if at all, AI assistance changed decisions about diagnosis or treatment. In drug trials, the answer is relatively simple: patients in one group were assigned to a drug regimen, while the others were assigned to placebo. To answer this question in the case of AI systems may require building models to predict treatment or diagnostic decisions in both arms of the study and comparing them for salient differences. If AI assistance improves patient outcomes, researchers should ensure that these improvements are stable across time, patient characteristics, and across clinicians of different specializations or levels of experience. It is not enough to demonstrate an improvement in patient outcomes: effort should be made to identify the mechanism by which the improvement was made. The latter point is fairly commonplace in the health sciences: Federica Russo and Jon Williamson (2007) argue that probabilistic evidence for causal conclusions in the health sciences must always be buttressed with plausible biomedical mechanisms. However, this point takes on a somewhat different aspect for AI RCTs. In an AI RCT study, we are not looking for a biological pathway, but an institutional and procedural pathway: how did interacting with the algorithmic system change diagnostic and therapeutic practice? Knowledge of biological mechanisms and anatomical microstructures cannot answer this question. Rather, researchers would have to combine statistical and sociological/ethnographic methods to understand the effect AI intervention has on medical practice. Finally, even if the involvement of AI systems improves the health outcomes of patients, it could still be detrimental to the clinician-patient relationship (cf. Bjerring and Busch 2020; Grote and Berens 2020). For instance, if AI assistance speeds up the diagnostic process, this could come at the expense of the care component, even if speeding up the diagnostic process theoretically frees up resources for care work. Moreover, AI assistance might interfere with the trust relationship between the patient and the clinician. If the clinician tends to (over-)rely on diagnostic support, the patient may suspect that it is not the clinician, but the AI system that is in command. The bottom line is that while AI systems may contribute to the instrumental aim of medicine (curing disease), these algorithms may nevertheless negatively affect the social dynamic of clinical practice. As these aspects are difficult for RCTs to capture, accompanying qualitative studies may be called for. Conclusion In this article, we have done three things: we have analysed the rationale for medical AI RCTs and provided an overview over existing medical AI RCTs. On this basis, we considered different methodological challenges for AI RCTs, while pointing out ways to meet these challenges. The concerns and recommendations that we have made above are by no means decisive or exhaustive. Instead, our article is meant to stimulate methodological reasoning for RCT trials testing AI assistance. An issue we have not discussed, for instance, concerns the validation of AI systems that are not frozen after the training phase (cf. Topol 2020). ne or the other group. 